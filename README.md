# PySpark-web-rank  
This is a computer system Homework to use PySpark and Google Dataproc to write a wikipedia webpage rank program.  
## Webgraph on Internal Links  
I write a Spark program which takes the xml file you just ingested as input (enwiki_whole.xml), and generate a csv file which describes the webgraph of the internal links in Wikipedia.   
## Spark PageRank  
In this task, I implement the PageRank algorithm, which Google uses to rank the website in the Google Search. We will use it to calculate the rank of the articles in Wikipedia. The algorithm can be summarized as follows:
- Each article has an initial rank of 1.  
- On each iteration, the contribution of an article A to its neighbor B is calculated as its rank / # of neighbors.  
- Update the rank of the article B to be 0.15 + 0.85 * contribution  
- Go to the next iteration.  
## Spark Streaming  
In this task,I write a stream receiver using Spark Structured Streaming to read file source while the file is being generated. The input source should be csv files. The receiver should keep a list of articles whose rank is greater than 0.5 and store the file inside the HDFS. The list should be updated dynamically while the Pagerank program is dumping the output and should be saved inside the HDFS.  
## Stream Emitter  
In this task, I write a stream emitter using Spark Streaming to emit the ‘txt’ file output generated by the PageRank program to a directory to emulate the case. Run the stream receiver to catch the stream which are emitting and check if I get the same result as former task.  

