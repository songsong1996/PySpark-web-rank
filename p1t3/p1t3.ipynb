{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.rank>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define UDFs\n",
    "def contribution(rank, n_neighbors):\n",
    "    ''' \n",
    "    Calculate the contribution A giving to B.\n",
    "    '''\n",
    "    contribute = rank / n_neighbors\n",
    "    return contribute\n",
    "\n",
    "spark.udf.register(\"contribution\", contribution)\n",
    "\n",
    "\n",
    "def rank(total_contribution):\n",
    "    '''\n",
    "    Calculate the rank of B.\n",
    "    '''\n",
    "    rank = total_contribution * 0.85 + 0.15\n",
    "    return rank\n",
    "\n",
    "spark.udf.register(\"rank\", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "articles = spark.read.format('csv').option(\"delimiter\", '\\t').load('hdfs:/p1t2_small.csv', header = 'true').withColumnRenamed('title', 'A').withColumnRenamed('col', 'B')\n",
    "# Initialize the rank of article to be 1\n",
    "ranks = articles.withColumn('Rank', lit(1))\n",
    "# Count the number of neighbors of A\n",
    "n_neighbors = ranks.groupBy('A').count().select(col('A').alias('A_'), 'count')\n",
    "# Save as a csv file\n",
    "ranks.write.format('com.databricks.spark.csv').option(\"delimiter\", '\\t').save('hdfs:/ranks_0.csv', header = 'true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time for loop', 1, ':', 36.26224708557129)\n",
      "('Time for loop', 2, ':', 38.694464921951294)\n",
      "('Time for loop', 3, ':', 38.74309802055359)\n",
      "('Time for loop', 4, ':', 38.30040097236633)\n",
      "('Time for loop', 5, ':', 38.36105704307556)\n",
      "('Time for loop', 6, ':', 38.980501890182495)\n",
      "('Time for loop', 7, ':', 37.994728088378906)\n",
      "('Time for loop', 8, ':', 38.68395113945007)\n",
      "('Time for loop', 9, ':', 38.76624894142151)\n",
      "('Time for loop', 10, ':', 37.985934019088745)\n"
     ]
    }
   ],
   "source": [
    "# Iterations\n",
    "import times\n",
    "for i in range(10):\n",
    "    ti0 = time.time()\n",
    "    # Load the file\n",
    "    ranks_ = spark.read.format('csv').option(\"delimiter\", '\\t').load('hdfs:/ranks_' + str(i) + '.csv', header = 'true')\n",
    "    # Join ranks with n_neighbors and then calculate the contribution each A giving to each B\n",
    "    contribution_A_to_B = ranks_.join(n_neighbors, ranks_.A == n_neighbors.A_, 'left').select('A', 'B', 'count', 'Rank').withColumnRenamed('count', 'NeighborsNum_of_A').withColumn('Contribution_of_A_to_B', contribution(col('Rank'), col('NeighborsNum_of_A')))\n",
    "    # Calculate the total contribution B receiving\n",
    "    totle_contributions_to_B = contribution_A_to_B.groupBy('B').agg({'Contribution_of_A_to_B': 'sum'}).withColumnRenamed('sum(Contribution_of_A_to_B)', 'Total_Contribution')\n",
    "    # Calculate the updated rank of B\n",
    "    B_Rank = totle_contributions_to_B.withColumn('Updated_Rank', rank(col('Total_Contribution'))).withColumnRenamed('B', 'Article')\n",
    "    # Join contribution_A_to_B with B_Rank on contribution_A_to_B.A == B_Rank.Article\n",
    "    combine = contribution_A_to_B.join(B_Rank, contribution_A_to_B.A == B_Rank.Article, 'left')\n",
    "    # If A has an updated rank, then update it. If it doesn't, then still use the old rank, which is 1.\n",
    "    update_A_Rank = combine.withColumn('New_Rank', when(col('Updated_Rank').isNull(), col('Rank')).otherwise(col('Updated_Rank')))\n",
    "    # Update ranks for next iteration\n",
    "    ranks_ = update_A_Rank.select('A', 'B', 'New_Rank').withColumnRenamed('New_Rank', 'Rank')\n",
    "    # Save as a csv file\n",
    "    ranks_.write.format('com.databricks.spark.csv').option(\"delimiter\", '\\t').save('hdfs:/ranks_'+ str(i + 1) +'.csv', header = 'true')\n",
    "    ti1 = time.time()\n",
    "    print(\"Time for loop\", i + 1, \":\", ti1 - ti0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv file\n",
    "# combine: contains the article names in the right column (B) of the input file and the corresponding ranks\n",
    "combine.write.format('com.databricks.spark.csv').option(\"delimiter\", '\\t').save('hdfs:/combine.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "combine_ = spark.read.format('csv').option(\"delimiter\", '\\t').load('hdfs:/combine.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If A has an updated rank, then update it. If it doesn't, then assign 0.15 to be its final rank.\n",
    "updated_ranks = combine_.withColumn('Rank', when(col('Updated_Rank').isNull(), 0.15).otherwise(col('Updated_Rank'))).select('A', 'Rank').withColumnRenamed('A', 'Article').distinct()\n",
    "\n",
    "# Sort the output by 'Article' and 'Rank' in ascending order.\n",
    "output = updated_ranks.orderBy(updated_ranks.Article.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the first five rows as a .csv file\n",
    "spark.createDataFrame(output.take(5)).write.format('com.databricks.spark.csv').option(\"delimiter\", '\\t').save('hdfs:/p1t3_small.csv', header = 'true')\n",
    "\n",
    "# Save as a .csv file\n",
    "# output.write.format('com.databricks.spark.csv').option(\"delimiter\", '\\t').save('hdfs:/p1t3.csv', header = 'true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}